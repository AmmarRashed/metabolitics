
 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.96      0.93      0.95        28
          h       0.88      0.93      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.82      0.96      0.89        28
          h       0.90      0.60      0.72        15

avg / total       0.85      0.84      0.83        43

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       1.00      0.96      0.98        27
          h       0.94      1.00      0.97        15

avg / total       0.98      0.98      0.98        42

train accuracy: 1.000000
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.96      0.93      0.94        27
          h       0.88      0.93      0.90        15

avg / total       0.93      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.92857143]
mean: 0.916258934864
std: 0.0452523464418
kfold test f1_micro: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.92857143]
mean: 0.916258934864
std: 0.0452523464418

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', MLPClassifier(activation='logistic', alpha=0.0001...
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,
       warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        28
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.790698

              precision    recall  f1-score   support

         bc       0.79      0.93      0.85        28
          h       0.80      0.53      0.64        15

avg / total       0.79      0.79      0.78        43

train accuracy: 1.000000
test accuracy: 1.000000

              precision    recall  f1-score   support

         bc       1.00      1.00      1.00        27
          h       1.00      1.00      1.00        15

avg / total       1.00      1.00      1.00        42

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       0.96      1.00      0.98        27
          h       1.00      0.93      0.97        15

avg / total       0.98      0.98      0.98        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', MLPClassifier(activation='logistic', alpha=0.0001...
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,
       warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.79069767  1.          0.97619048]
mean: 0.921242323568
std: 0.0727907379913
kfold test f1_micro: [ 0.90909091  0.93023256  0.79069767  1.          0.97619048]
mean: 0.921242323568
std: 0.0727907379913

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        28
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.82      0.96      0.89        28
          h       0.90      0.60      0.72        15

avg / total       0.85      0.84      0.83        43

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       1.00      0.96      0.98        27
          h       0.94      1.00      0.97        15

avg / total       0.98      0.98      0.98        42

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       0.96      1.00      0.98        27
          h       1.00      0.93      0.97        15

avg / total       0.98      0.98      0.98        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962
kfold test f1_micro: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f3e99de46a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.888235
test accuracy: 0.795455

              precision    recall  f1-score   support

         bc       0.81      0.89      0.85        28
          h       0.77      0.62      0.69        16

avg / total       0.79      0.80      0.79        44

train accuracy: 0.865497
test accuracy: 0.883721

              precision    recall  f1-score   support

         bc       0.87      0.96      0.92        28
          h       0.92      0.73      0.81        15

avg / total       0.89      0.88      0.88        43

train accuracy: 0.906433
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.75      0.86      0.80        28
          h       0.64      0.47      0.54        15

avg / total       0.71      0.72      0.71        43

train accuracy: 0.843023
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42

train accuracy: 0.848837
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.90      1.00      0.95        27
          h       1.00      0.80      0.89        15

avg / total       0.94      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f3e99de46a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446
kfold test f1_micro: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=20, score_func=<function f_classif at 0x7f84fc3b76a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.888235
test accuracy: 0.795455

              precision    recall  f1-score   support

         bc       0.81      0.89      0.85        28
          h       0.77      0.62      0.69        16

avg / total       0.79      0.80      0.79        44

train accuracy: 0.865497
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.89      0.86      0.87        28
          h       0.75      0.80      0.77        15

avg / total       0.84      0.84      0.84        43

train accuracy: 0.906433
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.75      0.86      0.80        28
          h       0.64      0.47      0.54        15

avg / total       0.71      0.72      0.71        43

train accuracy: 0.837209
test accuracy: 0.857143

              precision    recall  f1-score   support

         bc       0.96      0.81      0.88        27
          h       0.74      0.93      0.82        15

avg / total       0.88      0.86      0.86        42

train accuracy: 0.848837
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        27
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=20, score_func=<function f_classif at 0x7f84fc3b76a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.79545455  0.8372093   0.72093023  0.85714286  0.92857143]
mean: 0.827861673211
std: 0.0686611617391
kfold test f1_micro: [ 0.79545455  0.8372093   0.72093023  0.85714286  0.92857143]
mean: 0.827861673211
std: 0.0686611617391

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=50, score_func=<function f_classif at 0x7ff2ea8626a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.923529
test accuracy: 0.840909

              precision    recall  f1-score   support

         bc       0.84      0.93      0.88        28
          h       0.85      0.69      0.76        16

avg / total       0.84      0.84      0.84        44

train accuracy: 0.900585
test accuracy: 0.790698

              precision    recall  f1-score   support

         bc       0.77      0.96      0.86        28
          h       0.88      0.47      0.61        15

avg / total       0.81      0.79      0.77        43

train accuracy: 0.929825
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.74      0.89      0.81        28
          h       0.67      0.40      0.50        15

avg / total       0.71      0.72      0.70        43

train accuracy: 0.901163
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42

train accuracy: 0.912791
test accuracy: 0.904762

              precision    recall  f1-score   support

         bc       0.93      0.93      0.93        27
          h       0.87      0.87      0.87        15

avg / total       0.90      0.90      0.90        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=50, score_func=<function f_classif at 0x7ff2ea8626a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.84090909  0.79069767  0.72093023  0.95238095  0.9047619 ]
mean: 0.841935971006
std: 0.0817483921804
kfold test f1_micro: [ 0.84090909  0.79069767  0.72093023  0.95238095  0.9047619 ]
mean: 0.841935971006
std: 0.0817483921804

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('vt', VarianceThreshold(threshold=0.1)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f2d8c8356a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_comp...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.888235
test accuracy: 0.795455

              precision    recall  f1-score   support

         bc       0.81      0.89      0.85        28
          h       0.77      0.62      0.69        16

avg / total       0.79      0.80      0.79        44

train accuracy: 0.865497
test accuracy: 0.883721

              precision    recall  f1-score   support

         bc       0.87      0.96      0.92        28
          h       0.92      0.73      0.81        15

avg / total       0.89      0.88      0.88        43

train accuracy: 0.906433
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.75      0.86      0.80        28
          h       0.64      0.47      0.54        15

avg / total       0.71      0.72      0.71        43

train accuracy: 0.843023
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42

train accuracy: 0.848837
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.90      1.00      0.95        27
          h       1.00      0.80      0.89        15

avg / total       0.94      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('vt', VarianceThreshold(threshold=0.1)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f2d8c8356a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_comp...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446
kfold test f1_micro: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446

 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7f04ce3ca630>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DummyClassifier(constant=None, random_state=None, strategy='uniform'))]) 

train accuracy: 0.340836
test accuracy: 0.287500

              precision    recall  f1-score   support

         bc       0.38      0.32      0.35        28
          h       0.33      0.33      0.33        30
        hcc       0.38      0.45      0.42        22

avg / total       0.36      0.36      0.36        80

train accuracy: 0.323718
test accuracy: 0.265823

              precision    recall  f1-score   support

         bc       0.30      0.29      0.29        28
          h       0.34      0.33      0.34        30
        hcc       0.30      0.33      0.32        21

avg / total       0.32      0.32      0.32        79

train accuracy: 0.290735
test accuracy: 0.346154

              precision    recall  f1-score   support

         bc       0.36      0.32      0.34        28
          h       0.38      0.41      0.39        29
        hcc       0.52      0.52      0.52        21

avg / total       0.41      0.41      0.41        78

train accuracy: 0.321656
test accuracy: 0.324675

              precision    recall  f1-score   support

         bc       0.34      0.37      0.36        27
          h       0.33      0.38      0.35        29
        hcc       0.33      0.24      0.28        21

avg / total       0.34      0.34      0.33        77

train accuracy: 0.372611
test accuracy: 0.363636

              precision    recall  f1-score   support

         bc       0.36      0.37      0.36        27
          h       0.20      0.14      0.16        29
        hcc       0.21      0.29      0.24        21

avg / total       0.26      0.26      0.25        77


 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7f04fcb30f60>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DummyClassifier(constant=None, random_state=None, strategy='uniform'))]) 

kfold test accuracy: [ 0.3375      0.35443038  0.38461538  0.36363636  0.33766234]
mean: 0.355568893132
std: 0.0176477114004
kfold test f1_micro: [ 0.3375      0.35443038  0.38461538  0.36363636  0.33766234]
mean: 0.355568893132
std: 0.0176477114004

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        28
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.82      0.96      0.89        28
          h       0.90      0.60      0.72        15

avg / total       0.85      0.84      0.83        43

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       1.00      0.96      0.98        27
          h       0.94      1.00      0.97        15

avg / total       0.98      0.98      0.98        42

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       0.96      1.00      0.98        27
          h       1.00      0.93      0.97        15

avg / total       0.98      0.98      0.98        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962
kfold test f1_micro: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962

 Pipeline(steps=[('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('scaler', MetabolicStandardScaler()), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticReg...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.994118
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.96      0.89      0.93        28
          h       0.83      0.94      0.88        16

avg / total       0.91      0.91      0.91        44

train accuracy: 0.988304
test accuracy: 1.000000

              precision    recall  f1-score   support

         bc       1.00      1.00      1.00        28
          h       1.00      1.00      1.00        15

avg / total       1.00      1.00      1.00        43

train accuracy: 0.988304
test accuracy: 0.953488

              precision    recall  f1-score   support

         bc       0.93      1.00      0.97        28
          h       1.00      0.87      0.93        15

avg / total       0.96      0.95      0.95        43

train accuracy: 0.988372
test accuracy: 1.000000

              precision    recall  f1-score   support

         bc       1.00      1.00      1.00        27
          h       1.00      1.00      1.00        15

avg / total       1.00      1.00      1.00        42

train accuracy: 0.994186
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42


 Pipeline(steps=[('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('scaler', MetabolicStandardScaler()), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticReg...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  1.          0.95348837  1.          0.95238095]
mean: 0.962992046713
std: 0.0341977338552
kfold test f1_micro: [ 0.90909091  1.          0.95348837  1.          0.95238095]
mean: 0.962992046713
std: 0.0341977338552

 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7f04ccbcc710>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random...bf',
  max_iter=-1, probability=False, random_state=0, shrinking=True,
  tol=0.001, verbose=False))]) 

train accuracy: 0.665595
test accuracy: 0.012500

              precision    recall  f1-score   support

         bc       0.00      0.00      0.00        28
          h       0.00      0.00      0.00        30
        hcc       1.00      0.05      0.09        22

avg / total       0.28      0.01      0.02        80

train accuracy: 0.560897
test accuracy: 0.341772

              precision    recall  f1-score   support

         bc       0.42      0.75      0.54        28
          h       0.18      0.17      0.17        30
        hcc       1.00      0.05      0.09        21

avg / total       0.48      0.34      0.28        79

train accuracy: 0.472843
test accuracy: 0.435897

              precision    recall  f1-score   support

         bc       0.67      1.00      0.80        28
          h       0.00      0.00      0.00        29
        hcc       0.29      0.29      0.29        21

avg / total       0.32      0.44      0.36        78

train accuracy: 0.550955
test accuracy: 0.454545

              precision    recall  f1-score   support

         bc       0.75      1.00      0.86        27
          h       0.28      0.28      0.28        29
        hcc       0.00      0.00      0.00        21

avg / total       0.37      0.45      0.40        77

train accuracy: 0.531847
test accuracy: 0.350649

              precision    recall  f1-score   support

         bc       0.42      1.00      0.59        27
          h       0.00      0.00      0.00        29
        hcc       0.00      0.00      0.00        21

avg / total       0.15      0.35      0.21        77


 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7f04cdc1a128>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random...bf',
  max_iter=-1, probability=False, random_state=0, shrinking=True,
  tol=0.001, verbose=False))]) 

kfold test accuracy: [ 0.0125      0.34177215  0.43589744  0.45454545  0.35064935]
mean: 0.319072878598
std: 0.159687229614
kfold test f1_micro: [ 0.0125      0.34177215  0.43589744  0.45454545  0.35064935]
mean: 0.319072878598
std: 0.159687229614

 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7fcf700909b0>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DummyClassifier(constant=None, random_state=None, strategy='uniform'))]) 

train accuracy: 0.360129
test accuracy: 0.350000

              precision    recall  f1-score   support

         bc       0.35      0.25      0.29        28
          h       0.38      0.40      0.39        30
        hcc       0.25      0.32      0.28        22

avg / total       0.33      0.33      0.32        80

train accuracy: 0.323718
test accuracy: 0.278481

              precision    recall  f1-score   support

         bc       0.25      0.25      0.25        28
          h       0.28      0.17      0.21        30
        hcc       0.24      0.38      0.30        21

avg / total       0.26      0.25      0.25        79

train accuracy: 0.351438
test accuracy: 0.333333

              precision    recall  f1-score   support

         bc       0.42      0.36      0.38        28
          h       0.35      0.31      0.33        29
        hcc       0.29      0.38      0.33        21

avg / total       0.36      0.35      0.35        78

train accuracy: 0.328025
test accuracy: 0.402597

              precision    recall  f1-score   support

         bc       0.46      0.48      0.47        27
          h       0.29      0.24      0.26        29
        hcc       0.20      0.24      0.22        21

avg / total       0.33      0.32      0.32        77

train accuracy: 0.353503
test accuracy: 0.298701

              precision    recall  f1-score   support

         bc       0.34      0.41      0.37        27
          h       0.33      0.24      0.28        29
        hcc       0.21      0.24      0.22        21

avg / total       0.30      0.30      0.30        77


 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7fcf9e45a550>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DummyClassifier(constant=None, random_state=None, strategy='uniform'))]) 

kfold test accuracy: [ 0.3125      0.32911392  0.29487179  0.31168831  0.2987013 ]
mean: 0.309375065862
std: 0.0120748748816
kfold test f1_micro: [ 0.3125      0.32911392  0.29487179  0.31168831  0.2987013 ]
mean: 0.309375065862
std: 0.0120748748816

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        28
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.82      0.96      0.89        28
          h       0.90      0.60      0.72        15

avg / total       0.85      0.84      0.83        43

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       1.00      0.96      0.98        27
          h       0.94      1.00      0.97        15

avg / total       0.98      0.98      0.98        42

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       0.96      1.00      0.98        27
          h       1.00      0.93      0.97        15

avg / total       0.98      0.98      0.98        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962
kfold test f1_micro: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962

 Pipeline(steps=[('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('scaler', MetabolicStandardScaler()), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticReg...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.994118
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.96      0.89      0.93        28
          h       0.83      0.94      0.88        16

avg / total       0.91      0.91      0.91        44

train accuracy: 0.988304
test accuracy: 1.000000

              precision    recall  f1-score   support

         bc       1.00      1.00      1.00        28
          h       1.00      1.00      1.00        15

avg / total       1.00      1.00      1.00        43

train accuracy: 0.988304
test accuracy: 0.953488

              precision    recall  f1-score   support

         bc       0.93      1.00      0.97        28
          h       1.00      0.87      0.93        15

avg / total       0.96      0.95      0.95        43

train accuracy: 0.988372
test accuracy: 1.000000

              precision    recall  f1-score   support

         bc       1.00      1.00      1.00        27
          h       1.00      1.00      1.00        15

avg / total       1.00      1.00      1.00        42

train accuracy: 0.994186
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42


 Pipeline(steps=[('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('scaler', MetabolicStandardScaler()), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticReg...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  1.          0.95348837  1.          0.95238095]
mean: 0.962992046713
std: 0.0341977338552
kfold test f1_micro: [ 0.90909091  1.          0.95348837  1.          0.95238095]
mean: 0.962992046713
std: 0.0341977338552

 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7fcf70f1de10>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random...bf',
  max_iter=-1, probability=False, random_state=0, shrinking=True,
  tol=0.001, verbose=False))]) 

train accuracy: 0.665595
test accuracy: 0.012500

              precision    recall  f1-score   support

         bc       0.00      0.00      0.00        28
          h       0.00      0.00      0.00        30
        hcc       1.00      0.05      0.09        22

avg / total       0.28      0.01      0.02        80

train accuracy: 0.560897
test accuracy: 0.341772

              precision    recall  f1-score   support

         bc       0.42      0.75      0.54        28
          h       0.18      0.17      0.17        30
        hcc       1.00      0.05      0.09        21

avg / total       0.48      0.34      0.28        79

train accuracy: 0.472843
test accuracy: 0.435897

              precision    recall  f1-score   support

         bc       0.67      1.00      0.80        28
          h       0.00      0.00      0.00        29
        hcc       0.29      0.29      0.29        21

avg / total       0.32      0.44      0.36        78

train accuracy: 0.550955
test accuracy: 0.454545

              precision    recall  f1-score   support

         bc       0.75      1.00      0.86        27
          h       0.28      0.28      0.28        29
        hcc       0.00      0.00      0.00        21

avg / total       0.37      0.45      0.40        77

train accuracy: 0.531847
test accuracy: 0.350649

              precision    recall  f1-score   support

         bc       0.42      1.00      0.59        27
          h       0.00      0.00      0.00        29
        hcc       0.00      0.00      0.00        21

avg / total       0.15      0.35      0.21        77


 Pipeline(steps=[('scaler_most_active', <preprocessing.most_active_pathway_scaler.MostActivePathwayScaler object at 0x7fcf71148898>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random...bf',
  max_iter=-1, probability=False, random_state=0, shrinking=True,
  tol=0.001, verbose=False))]) 

kfold test accuracy: [ 0.0125      0.34177215  0.43589744  0.45454545  0.35064935]
mean: 0.319072878598
std: 0.159687229614
kfold test f1_micro: [ 0.0125      0.34177215  0.43589744  0.45454545  0.35064935]
mean: 0.319072878598
std: 0.159687229614
