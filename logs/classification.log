
 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.96      0.93      0.95        28
          h       0.88      0.93      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.82      0.96      0.89        28
          h       0.90      0.60      0.72        15

avg / total       0.85      0.84      0.83        43

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       1.00      0.96      0.98        27
          h       0.94      1.00      0.97        15

avg / total       0.98      0.98      0.98        42

train accuracy: 1.000000
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.96      0.93      0.94        27
          h       0.88      0.93      0.90        15

avg / total       0.93      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.92857143]
mean: 0.916258934864
std: 0.0452523464418
kfold test f1_micro: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.92857143]
mean: 0.916258934864
std: 0.0452523464418

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', MLPClassifier(activation='logistic', alpha=0.0001...
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,
       warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        28
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.790698

              precision    recall  f1-score   support

         bc       0.79      0.93      0.85        28
          h       0.80      0.53      0.64        15

avg / total       0.79      0.79      0.78        43

train accuracy: 1.000000
test accuracy: 1.000000

              precision    recall  f1-score   support

         bc       1.00      1.00      1.00        27
          h       1.00      1.00      1.00        15

avg / total       1.00      1.00      1.00        42

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       0.96      1.00      0.98        27
          h       1.00      0.93      0.97        15

avg / total       0.98      0.98      0.98        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', MLPClassifier(activation='logistic', alpha=0.0001...
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,
       warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.79069767  1.          0.97619048]
mean: 0.921242323568
std: 0.0727907379913
kfold test f1_micro: [ 0.90909091  0.93023256  0.79069767  1.          0.97619048]
mean: 0.921242323568
std: 0.0727907379913

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 1.000000
test accuracy: 0.909091

              precision    recall  f1-score   support

         bc       0.90      0.96      0.93        28
          h       0.93      0.81      0.87        16

avg / total       0.91      0.91      0.91        44

train accuracy: 1.000000
test accuracy: 0.930233

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        28
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        43

train accuracy: 1.000000
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.82      0.96      0.89        28
          h       0.90      0.60      0.72        15

avg / total       0.85      0.84      0.83        43

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       1.00      0.96      0.98        27
          h       0.94      1.00      0.97        15

avg / total       0.98      0.98      0.98        42

train accuracy: 1.000000
test accuracy: 0.976190

              precision    recall  f1-score   support

         bc       0.96      1.00      0.98        27
          h       1.00      0.93      0.97        15

avg / total       0.98      0.98      0.98        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=0.001, class_weight=None, du...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962
kfold test f1_micro: [ 0.90909091  0.93023256  0.8372093   0.97619048  0.97619048]
mean: 0.925782744387
std: 0.0514306361962

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f3e99de46a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.888235
test accuracy: 0.795455

              precision    recall  f1-score   support

         bc       0.81      0.89      0.85        28
          h       0.77      0.62      0.69        16

avg / total       0.79      0.80      0.79        44

train accuracy: 0.865497
test accuracy: 0.883721

              precision    recall  f1-score   support

         bc       0.87      0.96      0.92        28
          h       0.92      0.73      0.81        15

avg / total       0.89      0.88      0.88        43

train accuracy: 0.906433
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.75      0.86      0.80        28
          h       0.64      0.47      0.54        15

avg / total       0.71      0.72      0.71        43

train accuracy: 0.843023
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42

train accuracy: 0.848837
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.90      1.00      0.95        27
          h       1.00      0.80      0.89        15

avg / total       0.94      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f3e99de46a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446
kfold test f1_micro: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=20, score_func=<function f_classif at 0x7f84fc3b76a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.888235
test accuracy: 0.795455

              precision    recall  f1-score   support

         bc       0.81      0.89      0.85        28
          h       0.77      0.62      0.69        16

avg / total       0.79      0.80      0.79        44

train accuracy: 0.865497
test accuracy: 0.837209

              precision    recall  f1-score   support

         bc       0.89      0.86      0.87        28
          h       0.75      0.80      0.77        15

avg / total       0.84      0.84      0.84        43

train accuracy: 0.906433
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.75      0.86      0.80        28
          h       0.64      0.47      0.54        15

avg / total       0.71      0.72      0.71        43

train accuracy: 0.837209
test accuracy: 0.857143

              precision    recall  f1-score   support

         bc       0.96      0.81      0.88        27
          h       0.74      0.93      0.82        15

avg / total       0.88      0.86      0.86        42

train accuracy: 0.848837
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.93      0.96      0.95        27
          h       0.93      0.87      0.90        15

avg / total       0.93      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=20, score_func=<function f_classif at 0x7f84fc3b76a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.79545455  0.8372093   0.72093023  0.85714286  0.92857143]
mean: 0.827861673211
std: 0.0686611617391
kfold test f1_micro: [ 0.79545455  0.8372093   0.72093023  0.85714286  0.92857143]
mean: 0.827861673211
std: 0.0686611617391

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=50, score_func=<function f_classif at 0x7ff2ea8626a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.923529
test accuracy: 0.840909

              precision    recall  f1-score   support

         bc       0.84      0.93      0.88        28
          h       0.85      0.69      0.76        16

avg / total       0.84      0.84      0.84        44

train accuracy: 0.900585
test accuracy: 0.790698

              precision    recall  f1-score   support

         bc       0.77      0.96      0.86        28
          h       0.88      0.47      0.61        15

avg / total       0.81      0.79      0.77        43

train accuracy: 0.929825
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.74      0.89      0.81        28
          h       0.67      0.40      0.50        15

avg / total       0.71      0.72      0.70        43

train accuracy: 0.901163
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42

train accuracy: 0.912791
test accuracy: 0.904762

              precision    recall  f1-score   support

         bc       0.93      0.93      0.93        27
          h       0.87      0.87      0.87        15

avg / total       0.90      0.90      0.90        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('feature_selection', SelectKBest(k=50, score_func=<function f_classif at 0x7ff2ea8626a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solv...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.84090909  0.79069767  0.72093023  0.95238095  0.9047619 ]
mean: 0.841935971006
std: 0.0817483921804
kfold test f1_micro: [ 0.84090909  0.79069767  0.72093023  0.95238095  0.9047619 ]
mean: 0.841935971006
std: 0.0817483921804

 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('vt', VarianceThreshold(threshold=0.1)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f2d8c8356a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_comp...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

train accuracy: 0.888235
test accuracy: 0.795455

              precision    recall  f1-score   support

         bc       0.81      0.89      0.85        28
          h       0.77      0.62      0.69        16

avg / total       0.79      0.80      0.79        44

train accuracy: 0.865497
test accuracy: 0.883721

              precision    recall  f1-score   support

         bc       0.87      0.96      0.92        28
          h       0.92      0.73      0.81        15

avg / total       0.89      0.88      0.88        43

train accuracy: 0.906433
test accuracy: 0.720930

              precision    recall  f1-score   support

         bc       0.75      0.86      0.80        28
          h       0.64      0.47      0.54        15

avg / total       0.71      0.72      0.71        43

train accuracy: 0.843023
test accuracy: 0.952381

              precision    recall  f1-score   support

         bc       0.96      0.96      0.96        27
          h       0.93      0.93      0.93        15

avg / total       0.95      0.95      0.95        42

train accuracy: 0.848837
test accuracy: 0.928571

              precision    recall  f1-score   support

         bc       0.90      1.00      0.95        27
          h       1.00      0.80      0.89        15

avg / total       0.94      0.93      0.93        42


 Pipeline(steps=[('vect2', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=False)), ('vt', VarianceThreshold(threshold=0.1)), ('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x7f2d8c8356a8>)), ('pca', PCA(copy=True, iterated_power='auto', n_comp...ty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))]) 

kfold test accuracy: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446
kfold test f1_micro: [ 0.79545455  0.88372093  0.72093023  0.95238095  0.92857143]
mean: 0.85621161784
std: 0.086294519446
